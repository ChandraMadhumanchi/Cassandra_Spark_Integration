{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPq+kTP4gZBUy54I1dNXZIO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChandraMadhumanchi/Cassandra_Spark_Integration/blob/master/Restaurant_Predictiion_problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFy22oLNCs_S",
        "outputId": "d54736c1-c760-4330-f875-524775e25518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (3493, 34)\n",
            "Test shape: (500, 33)\n",
            "Numerical cols: 46 Categorical cols: 6\n",
            "small_card_cat: ['Restaurant Location', 'EndorsedBy', 'Restaurant Type']\n",
            "large_card_cat: ['Cuisine']\n",
            "high_card_cols: ['City', 'Restaurant Theme']\n",
            "Feature matrix shape: (3493, 52) Test matrix shape: (500, 52)\n",
            "Starting GridSearchCV for RandomForest...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "Best RF params: {'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 5, 'n_estimators': 400}\n",
            "Best RF CV RMSE: 0.48609021944375846\n",
            "Starting GridSearchCV for XGBoost...\n",
            "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
            "Best XGB params: {'colsample_bytree': 0.6, 'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 200, 'subsample': 0.9}\n",
            "Best XGB CV RMSE: 0.477815471791973\n",
            "Generating out-of-fold predictions for both models to estimate OOF RMSE...\n",
            "Best ensemble weight (XGB): 1.00, RF weight: 0.00, OOF RMSE: 20,540,684.90\n",
            "Fitting final models on full training data...\n",
            "Saved submission to: submission_rf_xgb.csv\n",
            "Submission head:\n",
            "    Registration Number  Annual Turnover\n",
            "0                20001         22731114\n",
            "1                20002         39451528\n",
            "2                20003         25789286\n",
            "3                20004         43786880\n",
            "4                20005         43757408\n",
            "Models dumped: best_rf.joblib, best_xgb.joblib\n",
            "Done. Summary:\n",
            "RF best params: {'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 5, 'n_estimators': 400}\n",
            "XGB best params: {'colsample_bytree': 0.6, 'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 200, 'subsample': 0.9}\n",
            "Estimated OOF ensemble RMSE: 20,540,684.90\n"
          ]
        }
      ],
      "source": [
        "# rf_xgb_pipeline.py\n",
        "import os\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import clone\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV, KFold, cross_val_predict\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer, LabelEncoder, OneHotEncoder, StandardScaler\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "# ---------- Utility functions ----------\n",
        "def rmse(a, b):\n",
        "    return np.sqrt(mean_squared_error(a, b))\n",
        "\n",
        "def log1p_and_back(y):\n",
        "    return np.log1p(y), lambda z: np.expm1(z)\n",
        "\n",
        "# ---------- Load data ----------\n",
        "TRAIN_PATH = \"Train_dataset_(1)_(1)_(2)_(1).csv\"\n",
        "TEST_PATH = \"Test_dataset_(1)_(1)_(2)_(1).csv\"\n",
        "OUTPUT_SUB = \"submission_rf_xgb.csv\"\n",
        "\n",
        "train = pd.read_csv(TRAIN_PATH)\n",
        "test = pd.read_csv(TEST_PATH)\n",
        "\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Test shape:\", test.shape)\n",
        "\n",
        "# keep registration ids\n",
        "test_ids = test['Registration Number'].values\n",
        "\n",
        "# ---------- Basic preprocessing & feature engineering ----------\n",
        "def preprocess_base(df, is_train=True):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Opening Day -> datetime features\n",
        "    if 'Opening Day of Restaurant' in df.columns:\n",
        "        df['Opening Day of Restaurant'] = pd.to_datetime(df['Opening Day of Restaurant'], errors='coerce')\n",
        "        df['Opening_Year'] = df['Opening Day of Restaurant'].dt.year.fillna(0).astype(int)\n",
        "        df['Opening_Month'] = df['Opening Day of Restaurant'].dt.month.fillna(0).astype(int)\n",
        "        df['Opening_DayOfYear'] = df['Opening Day of Restaurant'].dt.dayofyear.fillna(0).astype(int)\n",
        "        df['Days_Since_Opening'] = (pd.Timestamp.now() - df['Opening Day of Restaurant']).dt.days.fillna(0).astype(int)\n",
        "        # cyclical month\n",
        "        df['Opening_Month_Sin'] = np.sin(2 * np.pi * df['Opening_Month'] / 12)\n",
        "        df['Opening_Month_Cos'] = np.cos(2 * np.pi * df['Opening_Month'] / 12)\n",
        "        df.drop(columns=['Opening Day of Restaurant'], inplace=True, errors='ignore')\n",
        "\n",
        "    # Missing indicators for sparse event columns\n",
        "    for c in ['Live Music Rating', 'Comedy Gigs Rating', 'Value Deals Rating', 'Live Sports Rating', 'Overall Restaurant Rating']:\n",
        "        if c in df.columns:\n",
        "            df[c + '_NA'] = df[c].isna().astype(int)\n",
        "\n",
        "    # Social media / composite features\n",
        "    if 'Facebook Popularity Quotient' in df.columns and 'Instagram Popularity Quotient' in df.columns:\n",
        "        df['Social_Media_Score'] = (df['Facebook Popularity Quotient'].fillna(0) + df['Instagram Popularity Quotient'].fillna(0)) / 2\n",
        "        df['Social_Media_Ratio'] = (df['Facebook Popularity Quotient'].fillna(0) + 1) / (df['Instagram Popularity Quotient'].fillna(0) + 1)\n",
        "\n",
        "    # Service quality composite\n",
        "    service_cols = ['Staff Responsivness', 'Hygiene Rating', 'Food Rating', 'Service']\n",
        "    present = [c for c in service_cols if c in df.columns]\n",
        "    if present:\n",
        "        weights = {'Food Rating': 0.4, 'Service': 0.3, 'Hygiene Rating': 0.2, 'Staff Responsivness': 0.1}\n",
        "        df['Service_Quality_Score'] = 0.0\n",
        "        total_w = 0.0\n",
        "        for c in present:\n",
        "            w = weights.get(c, 0.25)\n",
        "            df['Service_Quality_Score'] += df[c].fillna(df[c].median()) * w\n",
        "            total_w += w\n",
        "        df['Service_Quality_Score'] /= total_w\n",
        "\n",
        "    # Entertainment / infrastructure / value features\n",
        "    ent_cols = [c for c in ['Live Music Rating','Comedy Gigs Rating','Ambience','Lively'] if c in df.columns]\n",
        "    if ent_cols:\n",
        "        df['Entertainment_Score'] = df[ent_cols].fillna(0).mean(axis=1)\n",
        "    infra_cols = [c for c in ['Fire Audit','Liquor License Obtained','Situated in a Multi Complex','Dedicated Parking','Open Sitting Available'] if c in df.columns]\n",
        "    if infra_cols:\n",
        "        df['Infrastructure_Score'] = df[infra_cols].fillna(0).sum(axis=1)\n",
        "\n",
        "    # Restaurant age info\n",
        "    if 'Days_Since_Opening' in df.columns:\n",
        "        df['Restaurant_Age_Years'] = df['Days_Since_Opening'] / 365.25\n",
        "        df['Is_New_Restaurant'] = (df['Restaurant_Age_Years'] < 1).astype(int)\n",
        "        df['Is_Established'] = (df['Restaurant_Age_Years'] > 5).astype(int)\n",
        "\n",
        "    # Value quality ratio\n",
        "    if 'Value for Money' in df.columns and 'Food Rating' in df.columns:\n",
        "        df['Value_Quality_Ratio'] = df['Value for Money'].fillna(df['Value for Money'].median()) / (df['Food Rating'].fillna(df['Food Rating'].median()) + 1)\n",
        "\n",
        "    # City frequency\n",
        "    if 'City' in df.columns:\n",
        "        df['City'] = df['City'].astype(str)\n",
        "        # frequency will be computed later with global mapping so fillna handled\n",
        "    return df\n",
        "\n",
        "# apply to combined to create consistent encodings\n",
        "combined = pd.concat([train.drop(columns=['Annual Turnover']), test], ignore_index=True, sort=False)\n",
        "combined = preprocess_base(combined, is_train=False)\n",
        "\n",
        "# compute City frequency on combined\n",
        "if 'City' in combined.columns:\n",
        "    city_counts = combined['City'].fillna('Unknown').value_counts()\n",
        "    combined['City_Frequency'] = combined['City'].fillna('Unknown').map(city_counts)\n",
        "\n",
        "# split back\n",
        "proc_train = combined.iloc[:len(train)].copy()\n",
        "proc_test  = combined.iloc[len(train):].copy()\n",
        "\n",
        "# Bring target back\n",
        "proc_train['Annual Turnover'] = train['Annual Turnover'].values\n",
        "\n",
        "# ---------- Prepare feature lists ----------\n",
        "drop_cols = ['Registration Number']  # keep id in test for submission\n",
        "target_col = 'Annual Turnover'\n",
        "\n",
        "# define categorical vs numerical\n",
        "all_cols = [c for c in proc_train.columns if c not in [target_col] + drop_cols]\n",
        "# heuristics for categorical\n",
        "cat_cols = [c for c in all_cols if proc_train[c].dtype == 'object']\n",
        "num_cols = [c for c in all_cols if c not in cat_cols]\n",
        "\n",
        "print(\"Numerical cols:\", len(num_cols), \"Categorical cols:\", len(cat_cols))\n",
        "\n",
        "# For high-cardinality categoricals, use frequency encoding (we already have City_Frequency)\n",
        "high_card_cols = []\n",
        "for c in cat_cols:\n",
        "    if proc_train[c].nunique() > 30:\n",
        "        high_card_cols.append(c)\n",
        "\n",
        "# We'll label-encode small categoricals and one-hot the truly small ones\n",
        "small_card_cat = [c for c in cat_cols if proc_train[c].nunique() <= 10]\n",
        "large_card_cat = [c for c in cat_cols if proc_train[c].nunique() > 10 and c not in high_card_cols]\n",
        "\n",
        "print(\"small_card_cat:\", small_card_cat)\n",
        "print(\"large_card_cat:\", large_card_cat)\n",
        "print(\"high_card_cols:\", high_card_cols)\n",
        "\n",
        "# ---------- Simple column transformer ----------\n",
        "# Imputation for numeric\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),  # scale for XGBoost sometimes helpful\n",
        "])\n",
        "\n",
        "# For categoricals: label encode small_card_cat, one-hot for small_card_cat with <=6 unique else ordinal via LabelEncoder\n",
        "# We'll implement a custom transformer using pandas operations for simplicity.\n",
        "\n",
        "def df_transform(X_df):\n",
        "    X = X_df.copy()\n",
        "    # label-encode small-cardinality categoricals\n",
        "    for c in small_card_cat:\n",
        "        X[c] = X[c].astype(str).fillna('NA_small')\n",
        "        le = LabelEncoder()\n",
        "        X[c] = le.fit_transform(X[c])\n",
        "    # label-encode large_card_cat (frequency encoding is often better, but keep label encoding)\n",
        "    for c in large_card_cat:\n",
        "        X[c] = X[c].astype(str).fillna('NA_large')\n",
        "        le = LabelEncoder()\n",
        "        X[c] = le.fit_transform(X[c])\n",
        "    # high-cardinality: frequency encoding\n",
        "    for c in high_card_cols:\n",
        "        freq = X[c].fillna('NA_high').value_counts()\n",
        "        X[c + \"_freq_enc\"] = X[c].fillna('NA_high').map(freq).fillna(0)\n",
        "        X.drop(columns=[c], inplace=True, errors='ignore')\n",
        "    # City already has City_Frequency - keep it and drop original\n",
        "    if 'City' in X.columns:\n",
        "        X.drop(columns=['City'], inplace=True, errors='ignore')\n",
        "    # fill any remaining missing numeric values with median\n",
        "    for c in X.columns:\n",
        "        if X[c].dtype in [np.float64, np.int64]:\n",
        "            X[c] = X[c].fillna(X[c].median())\n",
        "    return X\n",
        "\n",
        "# Apply df_transform\n",
        "X_train_raw = df_transform(proc_train[all_cols])\n",
        "X_test_raw  = df_transform(proc_test[all_cols])\n",
        "\n",
        "# Keep alignment of columns (some columns may be created/dropped)\n",
        "X_train_raw, X_test_raw = X_train_raw.align(X_test_raw, join='left', axis=1, fill_value=0)\n",
        "\n",
        "# separate features and target\n",
        "y = proc_train[target_col].values\n",
        "X = X_train_raw.values\n",
        "X_test = X_test_raw.values\n",
        "feature_names = X_train_raw.columns.tolist()\n",
        "\n",
        "print(\"Feature matrix shape:\", X.shape, \"Test matrix shape:\", X_test.shape)\n",
        "\n",
        "# ---------- Log transform target ----------\n",
        "y_log = np.log1p(y)\n",
        "\n",
        "# ---------- Cross-validated baseline (optional quick check) ----------\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "# ---------- Grid search for RandomForest ----------\n",
        "rf = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1)\n",
        "\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [200, 400],\n",
        "    'max_depth': [10, 15, None],\n",
        "    'min_samples_split': [5, 10],\n",
        "    'min_samples_leaf': [2, 5]\n",
        "}\n",
        "\n",
        "rf_gs = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=rf_param_grid,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "print(\"Starting GridSearchCV for RandomForest...\")\n",
        "rf_gs.fit(X, y_log)  # training on log target\n",
        "print(\"Best RF params:\", rf_gs.best_params_)\n",
        "print(\"Best RF CV RMSE:\", np.sqrt(-rf_gs.best_score_))\n",
        "\n",
        "best_rf = rf_gs.best_estimator_\n",
        "\n",
        "# ---------- Grid search for XGBoost ----------\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=RANDOM_STATE, n_jobs=-1, verbosity=0)\n",
        "\n",
        "xgb_param_grid = {\n",
        "    'n_estimators': [200, 400],\n",
        "    'max_depth': [4, 6, 8],\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'subsample': [0.7, 0.9],\n",
        "    'colsample_bytree': [0.6, 0.8]\n",
        "}\n",
        "\n",
        "xgb_gs = GridSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_grid=xgb_param_grid,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "print(\"Starting GridSearchCV for XGBoost...\")\n",
        "xgb_gs.fit(X, y_log)\n",
        "print(\"Best XGB params:\", xgb_gs.best_params_)\n",
        "print(\"Best XGB CV RMSE:\", np.sqrt(-xgb_gs.best_score_))\n",
        "\n",
        "best_xgb = xgb_gs.best_estimator_\n",
        "\n",
        "# ---------- Out-of-fold predictions to evaluate ensemble ----------\n",
        "print(\"Generating out-of-fold predictions for both models to estimate OOF RMSE...\")\n",
        "\n",
        "oof_rf = cross_val_predict(best_rf, X, y_log, cv=cv, n_jobs=-1)\n",
        "oof_xgb = cross_val_predict(best_xgb, X, y_log, cv=cv, n_jobs=-1)\n",
        "\n",
        "# convert back to original scale\n",
        "inv = lambda z: np.expm1(z)\n",
        "oof_rf_orig = inv(oof_rf)\n",
        "oof_xgb_orig = inv(oof_xgb)\n",
        "y_orig = y  # original\n",
        "\n",
        "# simple weighted ensemble search (grid search over weights)\n",
        "best_rmse = 1e18\n",
        "best_w = None\n",
        "for w in np.linspace(0, 1, 11):\n",
        "    pred = w * oof_xgb_orig + (1 - w) * oof_rf_orig\n",
        "    score = rmse(y_orig, pred)\n",
        "    if score < best_rmse:\n",
        "        best_rmse = score\n",
        "        best_w = w\n",
        "\n",
        "print(f\"Best ensemble weight (XGB): {best_w:.2f}, RF weight: {1-best_w:.2f}, OOF RMSE: {best_rmse:,.2f}\")\n",
        "\n",
        "# ---------- Fit final models on full training data ----------\n",
        "print(\"Fitting final models on full training data...\")\n",
        "best_rf.fit(X, y_log)\n",
        "best_xgb.fit(X, y_log)\n",
        "\n",
        "# ---------- Predict on test set ----------\n",
        "pred_rf_test_log = best_rf.predict(X_test)\n",
        "pred_xgb_test_log = best_xgb.predict(X_test)\n",
        "\n",
        "# convert back\n",
        "pred_rf_test = np.expm1(pred_rf_test_log)\n",
        "pred_xgb_test = np.expm1(pred_xgb_test_log)\n",
        "\n",
        "final_pred = best_w * pred_xgb_test + (1 - best_w) * pred_rf_test\n",
        "\n",
        "# clip negatives\n",
        "final_pred = np.clip(final_pred, a_min=0, a_max=None)\n",
        "\n",
        "# ---------- Submission ----------\n",
        "submission_df = pd.DataFrame({\n",
        "    \"Registration Number\": test_ids,\n",
        "    \"Annual Turnover\": np.round(final_pred).astype(int)\n",
        "})\n",
        "\n",
        "submission_df.to_csv(OUTPUT_SUB, index=False)\n",
        "print(\"Saved submission to:\", OUTPUT_SUB)\n",
        "print(\"Submission head:\\n\", submission_df.head())\n",
        "\n",
        "# ---------- Save models for later ----------\n",
        "joblib.dump(best_rf, \"best_rf.joblib\")\n",
        "joblib.dump(best_xgb, \"best_xgb.joblib\")\n",
        "print(\"Models dumped: best_rf.joblib, best_xgb.joblib\")\n",
        "\n",
        "# ---------- Final messages ----------\n",
        "print(\"Done. Summary:\")\n",
        "print(\"RF best params:\", rf_gs.best_params_)\n",
        "print(\"XGB best params:\", xgb_gs.best_params_)\n",
        "print(f\"Estimated OOF ensemble RMSE: {best_rmse:,.2f}\")\n"
      ]
    }
  ]
}